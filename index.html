<!DOCTYPE html>
<html lang='en'>

<head>
	<meta charset='UTF-8'>
	<meta name='viewport' content='width=device-width, initial-scale=1, shrink-to-fit=no'>
	<title>Zixian Ma</title>
	<meta http-equiv='X-UA-Compatible' content='IE=edge'>
	<link rel='stylesheet' href='asset/css/bootstrap.css'>
	<link rel='stylesheet' href='asset/css/all.css'>
	<link rel='stylesheet' href='asset/css/style.css'>
</head>

<body class='bg-light'>
	<header>
		<nav class='navbar navbar-light fixed-top bg-light'>
			<a class='navbar-brand col-sm-3 col-md-2 mt-1 ml-5' href='index.html'>
				<h2>Yash Garg</h2>
			</a>
			<ul class="nav justify-content-center">
				<li class="nav-item">
					<a class=" nav-link" href="asset\pdf\Yash_Garg_Resume16.pdf">CV</a>
				</li>
				<li class="nav-item">
					<a class=" nav-link" href="#research">Research</a>
				</li>
				<!-- <li class="nav-item">
					<a class=" nav-link" href="#project">Project</a>
				</li> -->
				<!-- <li class="nav-item">
					<a class=" nav-link" href="pages/dance.html">Dance</a>
				</li> -->
			</ul>
		</nav>
	</header>
	<section id="top">
		<div class='container pt-5'>
			<div class="row pt-5">
				<div class="col-3">
					<img class="img-fluid rounded" src="asset\image\my_photo.jpeg" alt="headshot">
				</div>
				<div class="col-9">
					<h4 class=''>Ph.D. Student @ University of California Riverside</h4>
					<!-- <h4 class=''>Computer Science B.S. and M.S. @ Stanford University </h4> -->
					<!-- <h4 class=''>Research Intern @ Google Research </h4> -->
					<div class="row">
						<p class="text-muted mx-3 my-2">ygarg [at] ucr.edu</p>
						<span class="icon"><a class="text-dark"
							href="https://scholar.google.com/citations?user=Lz5Jao4AAAAJ&hl=en"><i
								class="fas fa-graduation-cap mx-2"></i></a></span>
						<span class="icon"><a class="text-dark" href="https://github.com/yashgarg98"><i
							class="fab fa-github mx-2"></i></a></span>
						<span class="icon"><a class="text-dark" href="https://www.linkedin.com/in/yash-garg-881b73137/"><i
							class="fab fa-linkedin mx-2"></i></a></span>
				
					</div>
					<p>
						Hi! I am a PhD student (2023 - ?) at the University of California Riverside in the Electrical and Computer Engineering advised by <a class="" href="https://vcg.ece.ucr.edu/amit">Prof. Amit K. Roy-Chowdhury</a> and <a class="" href="https://intra.ece.ucr.edu/~sasif/">Prof. Salman Asif</a>. Between 2021 and 2023 I completed my Master's from University of California Riverside in the Electrical and Computer Engineering. 
						My current research interests lie broadly in ML, CV and 3D-vision, especially in 3D scene understanding, 3D human pose and shape estimation, multi-task and multi-domain learning, and parameter-efficient (PEFT) models.</p>
					</p>
				</div>
			</div>
			<!-- <div class="row pt-5">
					<p> My past projects cover multi-agent coordination, vision and languague models, and human-computer interaction.
						My current research interests lie in the intersection between AI/ML and HCI, especially human-AI interaction and collaboration. I'm broadly interested in these research questions:<br>
						<ul>
							<li class="bg-light">Human side: what do end users need when collaborating with AI? How can we make explanations helpful (if possible) in human-AI collaboration?</li>
							<li class="bg-light">AI/ML side: how do we develop ML models that can adapt to diverse user needs and behaviors?</li>
							<li class="bg-light">Interaction: what human-AI interactions are possible? How can we leverage interaction data to inform model design and development?</li>
						</ul>
					</p>
			</div> -->
		</div>
	</section>
	<!-- <section class="mx-5 my-5" id="highlights">
		<h3 class="mx-5">Recent highlights</h3>
		<ul class=" mx-5">
			<li class="bg-light">
				<b>[Aug, 2025] </b> Our paper <a href="https://arxiv.org/pdf/2412.05479">LATTE</a> has been accepted to EMNLP 2025 and selected for Oral presentation! See you in Suzhou :)
			</li>
			<li class="bg-light">
				<b>[May, 2025] </b> I'm honored to be a winner of the <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2025-north-america">Qualcomm Innovation Fellowship 2025</a> with <a href="https://yushi-hu.github.io/">Yushi Hu</a>!!
			</li>
			<li class="bg-light">
				<b>[Feb, 2025] </b> Our papers <a href="https://arxiv.org/pdf/2408.00754">Coarse Correspondences</a> and <a href="https://www.arxiv.org/abs/2506.07643">Synthetic Visual Genome</a> have been accepted to CVPR 2024. See you in Nashville!
			</li>
			<li class="bg-light">
				<b>[Sep, 2024] </b> Our papers <a href="https://www.task-me-anything.org/">Task Me Anything</a> and <a href="http://arxiv.org/abs/2410.14669">NaturalBench</a> have been accepted to NeurIPS 2024. See you in Vancouver!
			</li>
			<li class="bg-light">
				<b>[Jul, 2024] </b> Our benchmark <a href="https://mnms-project.github.io/">m&m's</a> has been accepted to ECCV 2024. See you in Milan! ðŸ˜‰
			</li>
			<li class="bg-light">
				<b>[Mar, 2024] </b> I'm co-organizing the <a href="https://syndata4cv.github.io/">Synthetic Data for Computer Vision Workshop @ CVPR 2024</a>!
			</li>
			<li class="bg-light">
				<b>[Sep, 2023] </b> Our paper <i>SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality</i> has been accepted to NeurIPS 2023 (Datasets & Benchmarks Track)!
			</li>
			<li class="bg-light">
				<b>[Mar, 2023] </b> Our paper <i>CREPE: Can Vision-Language Foundation Models Reason Compositionally?</i> has been accepted to CVPR 2023 and selected as a &#127775;<b>highlight</b>&#127775;!
			</li>
			<li class="bg-light">
				<b>[Jan, 2023] </b> Our paper <i>Model Sketching: Centering Concepts in Early-Stage Machine Learning Model Design</i> has been accepted to CHI 2023!
			</li>
		</ul>
		</div>
	</section> -->
	<!-- <section class="mx-5 my-5" id="research">
		<h3 class="mx-5">Selected publications</h3>
		<ul class="list-group list-group-flush mx-5">
			<h4 class="mt-3">Multi-modal Models</h4>
			<li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/taco.png" alt="task-me-anything teaser figure">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>LATTE: Learning to Think with Vision Specialists</h4>
						<p> <b>Zixian Ma</b>, Jianguo Zhang, Zhiwei Liu, Jieyu Zhang, Juntao Tan, Manli Shu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Caiming Xiong, Ranjay Krishna, Silvio Savarese</p>
						<p>EMNLP 2025 <span style="color:red;"><b>Oral Presentation</b></span> | SynData4CV workshop @ CVPR 2025</p>
						<p><a href="https://latte-web.github.io/">[Website]</a>
							<a href="https://arxiv.org/pdf/2412.05479">[PDF]</a>
							<a href="https://github.com/SalesforceAIResearch/LATTE"> [Code]</a>
						</p>
					</div>
				</div>
			</li>
			<li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/svg.png" alt="task-me-anything teaser figure">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>Synthetic Visual Genome</h4>
						<p>Jae Sung Park, <b>Zixian Ma</b>, Linjie Li, Chenhao Zheng, Cheng-Yu Hsieh, Ximing Lu, Khyathi Chandu, Quan Kong, Norimasa Kobori, Ali Farhadi, Yejin Choi, Ranjay Krishna</p>
						<p>CVPR 2025</p>
						<p><a href="https://synthetic-visual-genome.github.io/">[Website]</a>
							<a href="https://arxiv.org/abs/2506.07643">[PDF]</a>
							<a href="https://github.com/jamespark3922/SyntheticVG"> [Code]</a>
						</p>
					</div>
				</div>
			</li>
			<li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/task-me-anything.png" alt="task-me-anything teaser figure">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>Task Me Anything</h4>
						<p> Jieyu Zhang, Weikai Huang*, <u>Zixian Ma*</u>, Oscar Michel, Dong He, Tanmay Gupta, Wei-Chiu Ma, Ali Farhadi, Aniruddha Kembhavi, Ranjay Krishna.</p>
						<p>NeurIPS 2024 (Datasets & Benchmarks Track) | Video-Language Models @ NeurIPS 2024 <span style="color:red;"><b>Oral Presentation</b></span></p>
						<p><a href="https://www.task-me-anything.org/">[Website]</a>
							<a href="https://arxiv.org/abs/2406.11775">[PDF]</a>
							<a href="https://github.com/JieyuZ2/TaskMeAnything"> [Code]</a>
						</p>
					</div>
				</div>
			</li>
			<li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/natural_bench.png" alt="task-me-anything teaser figure">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples</h4>
						<p> Baiqi Li*, Zhiqiu Lin*, Wenxuan Peng*, Jean de Dieu Nyandwi*, Daniel Jiang, <u>Zixian Ma</u> Simran Khanuja, Ranjay Krishna, Graham Neubig, Deva Ramanan.</p>
						<p>NeurIPS 2024 (Datasets & Benchmarks Track)</p>
						<p><a href="http://arxiv.org/abs/2410.14669">[PDF]</a></p>
					</div>
				</div>
			</li>

			<li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/mnms_examples.png" alt="m&ms examples">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>m&m's: A Benchmark to Evaluate Tool-Use for <i>m</i>ulti-step <i>m</i>ulti-modal Tasks</h4>
						<p><u>Zixian Ma</u>, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna</p>
						<p>ECCV 2024 | SynData4CV workshop @ CVPR 2024</p>
						<p><a href="https://mnms-project.github.io/">[Website]</a>
							<a href="https://arxiv.org/pdf/2403.11085.pdf">[PDF]</a>
							<a href="https://github.com/RAIVNLab/mnms"> [Code]</a>
						</p>
					</div>
				</div>
			</li>
			<li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/sugarcrepe.png" alt="sugar crepe image">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality</h4>
						<p>Cheng-Yu Hsieh*, Jieyu Zhang*, <u>Zixian Ma</u>, Aniruddha Kembhavi, Ranjay Krishna</p>
						<p>NeurIPS 2023 (Datasets & Benchmarks Track)</p>
						<p><a href="https://arxiv.org/abs/2306.14610.pdf">[PDF]</a>
							<a href="https://github.com/RAIVNLab/sugar-crepe"> [Code]</a>
						</p>
					</div>
					</div>
			</li>
			<li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/crepe.png" alt="crepe image">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>CREPE: Can Vision-Language Foundation Models Reason Compositionally?</h4>
						<p><u>Zixian Ma*</u>, Jerry Hong*, Mustafa Omer Gul*, Mona Gandhi, Irena Gao, Ranjay Krishna</p>
						<p>CVPR 2023 <span style="color:red;"><b>Highlight [top 2.5%]</b></span></p>
						<p><a href="https://arxiv.org/pdf/2212.07796.pdf">[PDF]</a>
							<a href="https://github.com/RAIVNLab/CREPE"> [Code]</a>
						</p>
					</div>
				</div>
			</li>
		</ul>
		<ul class="list-group list-group-flush mx-5">
			<h4 class="mt-5">Human-AI Interaction</h4>
			
			<li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/model_sketching.png" alt="model sketching image">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>Model Sketching: Centering Concepts in Early-Stage Machine Learning Model Design</h4>
						<p>Michelle S. Lam, <u>Zixian Ma</u>, Anne Li, Izequiel Freitas, Dakuo Wang, James A. Landay, Michael S. Bernstein</p>
						<p>CHI 2023</p>
						<p><a href="https://arxiv.org/pdf/2303.02884.pdf">[PDF]</a></p>
					</div>
				</div>
			</li>
			<li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/align_2.png" alt="ELIGN image">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>ELIGN: Expection Alignment as a Multi-agent Intrinsic Reward</h4>
						<p><u>Zixian Ma</u>, Rose Wang, Li Fei-Fei, Michael Bernstein, Ranjay Krishna</p>
						<p>NeurIPS 2022</p>
						<p><a href="https://arxiv.org/pdf/2210.04365.pdf">[PDF]</a><a
								href="https://github.com/StanfordVL/alignment"> [Code]</a></p>
					</div>
				</div>
			</li>
		</ul>
		</div>
	</section> -->
	<hr class='featurette-divider'>
	<footer>
		<p class='mx-5 d-flex justify-content-end'><a class="" href='#top'>Back to top</a></p>
	</footer>
</body>

</html>